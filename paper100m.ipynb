{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data import AsNodePredDataset\n",
    "from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "import tqdm\n",
    "import argparse\n",
    "\n",
    "device = torch.device( 'cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # three-layer GraphSAGE-mean\n",
    "        self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, hid_size, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, out_size, 'mean'))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hid_size = hid_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            # import pdb; pdb.set_trace()\n",
    "            h = layer(block, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = F.relu(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, device, batch_size):\n",
    "        \"\"\"Conduct layer-wise inference to get all the node embeddings.\"\"\"\n",
    "        feat = g.ndata['feat']\n",
    "        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])\n",
    "        dataloader = DataLoader(\n",
    "                g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,\n",
    "                batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                num_workers=0)\n",
    "        buffer_device = torch.device('cpu')\n",
    "        pin_memory = (buffer_device != device)\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = torch.empty(\n",
    "                g.num_nodes(), self.hid_size if l != len(self.layers) - 1 else self.out_size,\n",
    "                device=buffer_device, pin_memory=pin_memory)\n",
    "            feat = feat.to(device)\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                x = feat[input_nodes]\n",
    "                h = layer(blocks[0], x) # len(blocks) = 1\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = F.relu(h)\n",
    "                    h = self.dropout(h)\n",
    "                # by design, our output nodes are contiguous\n",
    "                y[output_nodes[0]:output_nodes[-1]+1] = h.to(buffer_device)\n",
    "            feat = y\n",
    "        return y\n",
    "\n",
    "def evaluate(model, graph, dataloader):\n",
    "    dataloader.device = device\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    y_hats = []\n",
    "    for it, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            x = blocks[0].srcdata['feat']\n",
    "            ys.append(blocks[-1].dstdata['label'])\n",
    "            y_hats.append(model(blocks, x))\n",
    "    return MF.accuracy(torch.cat(y_hats), torch.cat(ys),task='multiclass',num_classes=172)\n",
    "\n",
    "def layerwise_infer(device, graph, nid, model, batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.inference(graph, device, batch_size) # pred in buffer_device\n",
    "        pred = pred[nid]\n",
    "        label = graph.ndata['label'][nid].to(pred.device)\n",
    "        return MF.accuracy(pred, label,task='multiclass',num_classes=172)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 52.96 GiB (GPU 0; 39.44 GiB total capacity; 24.08 GiB already allocated; 14.71 GiB free; 24.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice( \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# create GraphSAGE model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m in_size \u001b[39m=\u001b[39m g1\u001b[39m.\u001b[39;49mndata[\u001b[39m'\u001b[39;49m\u001b[39mfeat\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m out_size \u001b[39m=\u001b[39m dataset1\u001b[39m.\u001b[39mnum_classes\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39min_size \u001b[39m\u001b[39m'\u001b[39m, in_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/brt_env/lib/python3.10/site-packages/dgl/view.py:73\u001b[0m, in \u001b[0;36mHeteroNodeDataView.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[1;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49m_get_n_repr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ntid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nodes)[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/brt_env/lib/python3.10/site-packages/dgl/frame.py:622\u001b[0m, in \u001b[0;36mFrame.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    610\u001b[0m     \u001b[39m\"\"\"Return the column of the given name.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[1;32m    612\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39m        Column data.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_columns[name]\u001b[39m.\u001b[39;49mdata\n",
      "File \u001b[0;32m~/miniconda3/envs/brt_env/lib/python3.10/site-packages/dgl/frame.py:231\u001b[0m, in \u001b[0;36mColumn.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m# move data to the right device\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcopy_to(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m# convert data to the right type\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/brt_env/lib/python3.10/site-packages/dgl/backend/pytorch/tensor.py:117\u001b[0m, in \u001b[0;36mcopy_to\u001b[0;34m(input, ctx, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39mindex \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         th\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(ctx\u001b[39m.\u001b[39mindex)\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid context\u001b[39m\u001b[39m'\u001b[39m, ctx)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 52.96 GiB (GPU 0; 39.44 GiB total capacity; 24.08 GiB already allocated; 14.71 GiB free; 24.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "dataset1 = AsNodePredDataset(DglNodePropPredDataset('ogbn-papers100M'))\n",
    "g1 = dataset1[0]\n",
    "g1= g1.to('cuda')\n",
    "device = torch.device( 'cuda')\n",
    "\n",
    "# create GraphSAGE model\n",
    "# import pdb; pdb.set_trace()\n",
    "in_size = g1.ndata['feat'].shape[1]\n",
    "out_size = dataset1.num_classes\n",
    "print('in_size ', in_size)\n",
    "print('out_size ', out_size)\n",
    "model = SAGE(in_size, 256, out_size).to(device)\n",
    "print('device: ', device)\n",
    "\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# print('model device: ', model.device)\n",
    "\n",
    "# model training\n",
    "print('Training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, g, dataset, model):\n",
    "    # create sampler & dataloader\n",
    "    cpu_device=torch.device('cpu')\n",
    "    # print('dataset device: ', dataset.train_idx.device)\n",
    "    train_idx = dataset.train_idx.to(device)\n",
    "    val_idx = dataset.val_idx.to(device)\n",
    "    sampler = NeighborSampler([10, 10, 10],  # fanout for [layer-0, layer-1, layer-2]\n",
    "                              prefetch_node_feats=['feat'],\n",
    "                              prefetch_labels=['label'])\n",
    "    use_uva =True\n",
    "    train_dataloader = DataLoader(g, train_idx, sampler, device=device,\n",
    "                                  batch_size=102400, shuffle=True,\n",
    "                                  drop_last=False, num_workers=0,\n",
    "                                  use_uva=use_uva)\n",
    "\n",
    "    val_dataloader = DataLoader(g, val_idx, sampler, device=device,\n",
    "                                batch_size=102400, shuffle=True,\n",
    "                                drop_last=False, num_workers=0,\n",
    "                                use_uva=use_uva)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "    for epoch in range(1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for it, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "            print('it: ', it)\n",
    "            ## consider block device\n",
    "            blocks = [b.to(device) for b in blocks]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            x = blocks[0].srcdata['feat']\n",
    "            y = blocks[-1].dstdata['label'].to(torch.int64) \n",
    "            y_hat = model(blocks, x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        acc = evaluate(model, g, val_dataloader)\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \"\n",
    "              .format(epoch, total_loss / (it+1), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proflie=True\n",
    "if proflie==True:\n",
    "    prof = torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        profile_memory=True,\n",
    "        schedule=torch.profiler.schedule(wait=0, warmup=0, active=1, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/node_classification_dgl_bs102400_cuda_first'),\n",
    "        record_shapes=True,\n",
    "        with_stack=True)\n",
    "    prof.start()\n",
    "    # for step, batch_data in enumerate(train_loader):\n",
    "    #     if step >= (1 + 1 + 3) * 2:\n",
    "    #         break\n",
    "    #     train(batch_data)\n",
    "    #     prof.step()\n",
    "    train( device, g1, dataset1, model)\n",
    "    prof.stop()\n",
    "else:\n",
    "    train(device, g1, dataset1, model)\n",
    "\n",
    "\n",
    "# test the model\n",
    "print('Testing...')\n",
    "acc = layerwise_infer(device, g1, dataset1.test_idx, model, batch_size=4096)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ffc20edf0549fd7f79d2b78b336e5a238553ac407ce6895821e68280ec1a5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
